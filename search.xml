<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[t-SNE]]></title>
      <url>http://shuimuth.github.io/2016/05/23/tsne/</url>
      <content type="html"><![CDATA[<h1 id="t-distributed-stochastic-neighbor-embedding-t-sne">t-distributed stochastic neighbor embedding (t-SNE)</h1>
<p>为了探索为何经典的机器学习算法在噪声很大数据集中表现不佳，甚至是深度学习算法都无用武之地，所以深入剖析情感数据 集的内部结构是很有必要的。 主要分析方法是通过流形分析方法将高维数据映射到低维空间中，再用工具把数据可视化显示出来。另外，为了分析每一个特征与分类类标的相关性，也对每一个特征进行了单独分析。 <a id="more"></a> 在流形分析方法中主要用了 t-distributed stochastic neighbor embedding (t-SNE)，t-SNE 分析方法是 Hinton 老教授 2008 年在<a href="http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" target="_blank" rel="external">Visualizing High-Dimensional Data Using t-SNE</a>这篇文章中提出来的。t-SNE 是一种非线性降维方法，这种方法非常适合把嵌在高维空间中的数据映射到二维或者三维，从而可以用离散画图方法画出来。 这个方法就介绍了，具体参考前面给出的相关文献。下面看看 t-SNE 的能力如何吧，我们先在 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="external">mnist</a>数据集上做个测试，一下脚本参考 sklearn 的 <a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html" target="_blank" rel="external">Manifold learning on handwritten digits</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Authors: Fabian Pedregosa &lt;fabian.pedregosa@inria.fr&gt;</span></span><br><span class="line"><span class="comment">#          Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span></span><br><span class="line"><span class="comment">#          Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span></span><br><span class="line"><span class="comment">#          Gael Varoquaux</span></span><br><span class="line"><span class="comment"># License: BSD 3 clause (C) INRIA 2011</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> offsetbox</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> (manifold, datasets, decomposition, ensemble,</span><br><span class="line">                     discriminant_analysis, random_projection)</span><br><span class="line"></span><br><span class="line">digits = datasets.load_digits(n_class=<span class="number">6</span>)</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">n_samples, n_features = X.shape</span><br><span class="line">n_neighbors = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#----------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Scale and visualize the embedding vectors</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_embedding</span><span class="params">(X, title=None)</span>:</span></span><br><span class="line">    x_min, x_max = np.min(X, <span class="number">0</span>), np.max(X, <span class="number">0</span>)</span><br><span class="line">    X = (X - x_min) / (x_max - x_min)</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    ax = plt.subplot(<span class="number">111</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">        plt.text(X[i, <span class="number">0</span>], X[i, <span class="number">1</span>], str(digits.target[i]),</span><br><span class="line">                 color=plt.cm.Set1(y[i] / <span class="number">10.</span>),</span><br><span class="line">                 fontdict=&#123;<span class="string">'weight'</span>: <span class="string">'bold'</span>, <span class="string">'size'</span>: <span class="number">9</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> hasattr(offsetbox, <span class="string">'AnnotationBbox'</span>):</span><br><span class="line">        <span class="comment"># only print thumbnails with matplotlib &gt; 1.0</span></span><br><span class="line">        shown_images = np.array([[<span class="number">1.</span>, <span class="number">1.</span>]])  <span class="comment"># just something big</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(digits.data.shape[<span class="number">0</span>]):</span><br><span class="line">            dist = np.sum((X[i] - shown_images) ** <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> np.min(dist) &lt; <span class="number">4e-3</span>:</span><br><span class="line">                <span class="comment"># don't show points that are too close</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            shown_images = np.r_[shown_images, [X[i]]]</span><br><span class="line">            imagebox = offsetbox.AnnotationBbox(</span><br><span class="line">                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),</span><br><span class="line">                X[i])</span><br><span class="line">            ax.add_artist(imagebox)</span><br><span class="line">    plt.xticks([]), plt.yticks([])</span><br><span class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        plt.title(title)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#----------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># t-SNE embedding of the digits dataset</span></span><br><span class="line">print(<span class="string">"Computing t-SNE embedding"</span>)</span><br><span class="line">tsne = manifold.TSNE(n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, random_state=<span class="number">0</span>)</span><br><span class="line">t0 = time()</span><br><span class="line">X_tsne = tsne.fit_transform(X)</span><br><span class="line"></span><br><span class="line">plot_embedding(X_tsne,</span><br><span class="line">               <span class="string">"t-SNE embedding of the digits (time %.2fs)"</span> %</span><br><span class="line">               (time() - t0))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div align="center">
<img src="/2016/05/23/tsne/mnist.png" alt="mnist.png" title="">
</div>
<p>从上面的输出结果看 t-SNE 确实具有极强的流形分析能力，能够把 mnist 手写数据集这个比较复杂模式区分开来，所以这个非线 性的流形学习方法能够有助于我们发现数据集内部的模式，下面使用 t-SNE 方法分析老师所给数据集是否存在某种特定的模式</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Support Vector Machine]]></title>
      <url>http://shuimuth.github.io/2016/05/23/svm/</url>
      <content type="html"><![CDATA[<h1 id="support-vector-machine">Support Vector Machine</h1>
<h2 id="线性可分类">线性可分类</h2>
<p>这里只介绍两类的线性可分问题，我们的任务就是设计一个超平面来划分数据集。 设<span class="math inline">\(\mathbf{x}_i, i=1,2,\ldots,N\)</span>是训练集<span class="math inline">\(X\)</span>中的特征向量。这些类来自于类<span class="math inline">\(\omega_1, \omega_2\)</span>,并且假设是线性可分的。那么我们设计的超平面可以用这条公式来描述：</p>
<p><span class="math display">\[g(x) = \mathbf{\omega}^T \mathbf{x} + \omega_0\]</span></p>
<a id="more"></a>
<p>如果用感知器算法来解，可能会收敛到任何可能的解。 而对于这样的问题，得到的超平面不是唯一的，如下面这幅图所示。</p>
<div align="center">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Classifier.svg/150px-Classifier.svg.png" title="线性 可分二类问题">
</div>
<p>从上面的图来看，我们觉得用<span class="math inline">\(l_2\)</span>这条线作为决策边界会更好一点。因为这条线把两个类分得<strong>比较开</strong>。 基于这样的直觉，我们希望得到一条最优的<strong>分界线</strong>,而这条分界线是如何定义的，又是如何得到的，这就是 SVM 要解决的问题。</p>
<h2 id="问题的定义与求解">问题的定义与求解</h2>
<p>基于上面的介绍，我们现在的任务就是寻找一个方向使得分界线使得间隔尽可能大(具体解释可参考 Simon Haykin 教授的<strong>Neural Networks and Learning Machines</strong>),因为平面到点的距离可以通过下面这条公式求解</p>
<p><span class="math display">\[z = \frac{|g(x)|} {\|\mathbf{w}\|}\]</span></p>
<p>因为缩放参数<span class="math inline">\(\omega\)</span>不影响直线，现在缩放<span class="math inline">\(\omega, \omega_0\)</span>,使得点在<span class="math inline">\(\omega_1\)</span>和<span class="math inline">\(\omega_2\)</span>类中的间隔最近，<span class="math inline">\(g(x)\)</span>对于<span class="math inline">\(\omega_1\)</span>等于 1，对于<span class="math inline">\(\omega_2\)</span>等于-1，因而有可以进一步把问题用下面的方程来表示：</p>
<ol style="list-style-type: decimal">
<li>存在一个间隔，满足<span class="math display">\[\max_{\mathbf{w}} \frac{1}{\|\mathbf{w}\|}\]</span></li>
<li>要求满足约束</li>
</ol>
<p><span class="math display">\[\mathbf{w}^T \mathbf{x} + w_0 \ge 1, \quad \forall \mathbf{x} \in \omega_1 \\
\mathbf{w}^T \mathbf{x} + w_0 \le -1, \quad \forall \mathbf{x} \in \omega_2
\]</span></p>
<p>对于每一个<span class="math inline">\(\mathbf{x}_i\)</span>,用<span class="math inline">\(y_i\)</span>表示相对应的类标(对于<span class="math inline">\(\omega_1\)</span>为+1，对于<span class="math inline">\(\omega_2\)</span>为-1)，则现在可以简化为计算超平面参数<span class="math inline">\(\mathbf{w},w_0\)</span>:</p>
<p><span class="math display">\[\min_{(\mathbf{w}, w_0)} \mathcal{J}(\mathbf{w}, w_0) \equiv \frac{1}{2} \|\mathbf{w}\|^2 \\
\text{constraint to}. \quad y_i(\mathbf{w}^T\mathbf{x}_i + w_0) \ge 1, \quad i = 1,2,\ldots, N
\]</span></p>
<p>具体的公式介绍和推导可参考下面两本教材和博客，里面有非常详细的推导 Simon Haykin 教授的<strong>Neural Networks and Learning Machines</strong> Sergios Theodoridis 教授和 Konstantinos Koutroumbas 教授的<strong>Pattern Recognition</strong> <a href="http://blog.csdn.net/v_july_v/article/details/7624837" target="_blank" rel="external">支持向量机通俗导论</a></p>
<h2 id="线性不可分问题">线性不可分问题</h2>
<p>对于这个问题只把关键公式写出来，就不做详细分析了，主要想法就是引入松弛变量(slack variable)<span class="math inline">\(\xi_i\)</span>, 约束条件改为 <span class="math display">\[y_i(\mathbf{w}^T \mathbf{x} + w_0) \ge 1 - \xi_i\]</span> 最终解最小化代价函数</p>
<p><span class="math display">\[
\min \mathcal{J}(\mathbf{w}, w_0, \mathbf{\xi}) = \frac{1}{2} \|\mathbf{w}\|^2 + \mathcal{C} \sum_{i=1}^N {I(\xi_i)}\\
\text{subject to.} \quad y_i(\mathbf{w}^T \mathbf{x} + w_0) \ge 1 - \xi_i, \quad i=1,2,\ldots,N \\
\xi_i \ge 0, \quad i=1,2,\ldots,N
\]</span></p>
<p>其中，<span class="math inline">\(\mathbf{\xi}\)</span>是参数<span class="math inline">\(\xi_i\)</span>组成的向量，并且</p>
<p><span class="math display">\[I(\xi_i) =
\begin{cases}
1, \quad \xi_i \gt 0 \\
0, \quad \xi_i = 0
\end{cases}
\]</span></p>
<p>为了更清晰的了解支持向量机的工作机制，这里借用了 sklearn 中 svm 的例子，具体可参考<a href="http://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html" target="_blank" rel="external">svm margin example</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"></span><br><span class="line"><span class="comment"># we create 40 separable points</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.r_[np.random.randn(<span class="number">20</span>, <span class="number">2</span>) - [<span class="number">2</span>, <span class="number">2</span>], np.random.randn(<span class="number">20</span>, <span class="number">2</span>) + [<span class="number">2</span>, <span class="number">2</span>]]</span><br><span class="line">Y = [<span class="number">0</span>] * <span class="number">20</span> + [<span class="number">1</span>] * <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model</span></span><br><span class="line">clf = svm.SVC(kernel=<span class="string">'linear'</span>)</span><br><span class="line">clf.fit(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the separating hyperplane</span></span><br><span class="line">w = clf.coef_[<span class="number">0</span>]</span><br><span class="line">a = -w[<span class="number">0</span>] / w[<span class="number">1</span>]</span><br><span class="line">xx = np.linspace(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">yy = a * xx - (clf.intercept_[<span class="number">0</span>]) / w[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the parallels to the separating hyperplane that pass through the</span></span><br><span class="line"><span class="comment"># support vectors</span></span><br><span class="line">b = clf.support_vectors_[<span class="number">0</span>]</span><br><span class="line">yy_down = a * xx + (b[<span class="number">1</span>] - a * b[<span class="number">0</span>])</span><br><span class="line">b = clf.support_vectors_[<span class="number">-1</span>]</span><br><span class="line">yy_up = a * xx + (b[<span class="number">1</span>] - a * b[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the line, the points, and the nearest vectors to the plane</span></span><br><span class="line">plt.plot(xx, yy, <span class="string">'k-'</span>)</span><br><span class="line">plt.plot(xx, yy_down, <span class="string">'k--'</span>)</span><br><span class="line">plt.plot(xx, yy_up, <span class="string">'k--'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>],</span><br><span class="line">            s=<span class="number">80</span>, facecolors=<span class="string">'none'</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, cmap=plt.cm.Paired)</span><br><span class="line"></span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div align="center">
<img src="/2016/05/23/svm/sp.png" alt="sp.png" title="">
</div>
<p>要从底层一步步实现 SVM 还是挺难的，也挺繁琐，所以这次知识用了 python 的 sklearn 库，之后有时间再把具体的实现写下来，这里有一个博客<a href="http://blog.csdn.net/zouxy09/article/details/17292011" target="_blank" rel="external">机器学习算法与 Python 实践之（四）支持向量机（SVM）实现</a>写了 svm 的实现，到时候也可以参考参考</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Logistic Regression]]></title>
      <url>http://shuimuth.github.io/2016/05/22/logistic-regression/</url>
      <content type="html"><![CDATA[<h1 id="logistic-regression">Logistic Regression</h1>
<p>logistic regression 和 perceptron 其实是比较像的，都是只能对线性可分的数据建模，也是基于神经科学的神经元模型得来的，神经元模型图如下。不过 logistic regression 的输出结果值具有一定的解释性，可以把它理解为概率。另外一方面，logistic regression 可以很容易地扩展到对多个类建模，而 percptron 只能作为二元分类器。 logistic regression 使用 sigmoid 激活函数</p>
<p><span class="math display">\[\sigma = \frac{1}{1 + \exp^{-z}}\]</span> <a id="more"></a></p>
<p>如果更加深入的来说，对于输入为<span class="math inline">\(x_1,x_2,x_3,...\)</span>，权重为<span class="math inline">\(w_1,w_2,w_3,...\)</span>,偏置<span class="math inline">\(b\)</span>，则相对应的激活值为</p>
<p><span class="math display">\[\frac{1}{1 + \exp^{- \sum_i{w_ix_i} - b}}\]</span></p>
<div align="center" style="margin-bottom=20px">
<img src="/2016/05/22/logistic-regression/neuron_model.jpeg" alt="神经元模型" title="神经元模型">
</div>
<h3 id="学习准则">学习准则</h3>
<p>学习到的模型需要一个机制去评估参数<span class="math inline">\(w,b\)</span>是否比较好，所以说需要对我们做出的模型函数进行评估，一般这个函数称为损失函数（loss function）或者错误函数(error function)。假设模型拟合函数为<span class="math inline">\(h(\theta)\)</span>,那么在我们这个 logistic regression 模型中有<span class="math inline">\(h(\theta)=\sigma(w,b)\)</span>.描述模型函数<span class="math inline">\(h(\theta)\)</span>不好的程度，一般称这个函数为损失函数,通常情况下可以使用欧氏距离损失函数 <span class="math display">\[J(\theta)=\frac{1}{2} \sum_i^N{(h_{\theta}(x^i) - y^i)^2}\]</span> 只要最优化这个函数便可以得到在这个损失函数标准下的最好的模型。在机器学习方法中通常使用迭代的方法来逼近最有解，所以在这个算法里也使用了同样的思想。</p>
<h3 id="梯度下降">梯度下降</h3>
<p>在选定线性回归模型后，只需要确定参数<span class="math inline">\(\theta\)</span>，就可以将模型用来预测。然而θ需要在 <span class="math inline">\(J(\theta)\)</span>最小的情况下才能确定。因此问题归结为求极小值问题，使用梯度下降法。梯度下降法最大的问题是求得有可能是全局极小值，这与初始点的选取有关。 梯度下降法是按下面的流程进行的：</p>
<ol style="list-style-type: decimal">
<li>首先对<span class="math inline">\(\theta\)</span>赋值，这个值可以是随机的，也可以让<span class="math inline">\(\theta\)</span>是一个全零的向量。</li>
<li>改变<span class="math inline">\(\theta\)</span>的值，使得 <span class="math inline">\(J(\theta)\)</span>按梯度下降的方向进行减少。</li>
</ol>
<p>梯度方向由 <span class="math inline">\(J(\theta)\)</span>对<span class="math inline">\(\theta\)</span>的偏导数确定，由于求的是极小值，因此梯度方向是偏导数的反方向。结果为</p>
<p><span class="math display">\[\theta_j := \theta_j + \alpha(h_{\theta}(x^i - y^i))x_j^i\]</span></p>
<p>迭代更新的方式有两种，一种是批梯度下降，也就是对全部的训练数据求得误差后再对θ进行更新，另外一种是增量梯度下降，每扫描一步都要对θ进行更新。前一种方法能够不断收敛，后一种方法结果可能不断在收敛处徘徊。这次实现的算法中使用的是批梯度下降的方法。</p>
<h3 id="logistic-regression-背后的概率原理">Logistic Regression 背后的概率原理</h3>
<p>logistic regression 是人工神经网络中非常基础也是非常重要的一个模型，就算是深度学习中的深度神经网络中的决策层也 是经常使用 logistic 层或者<a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="external">softmax</a>层。在网上搜索基本上很少看到 有分析 logistic regression 原理性的中文博客文章，大多是直接上公式然后求梯度，只解释了怎么用，而不解释清楚什么情 况下可以用，为什么可以用，所以打算再深入的剖析 logistic regression 背后合理性，也加强自己的记忆。我们可能会问， 为何可以用<span class="math inline">\(\sigma\)</span>函数来做回归，并且符合哪些分布的数据用 logistic regression 能够得到较好的效果。 为了分析的简单，先分析两类的情况，其实也很容易拓展到多类。假设在每个类的条件下数据分布的密度函数为 <span class="math inline">\(p(\mathbf{x}|\mathcal{C}_k)\)</span>,每个类的先验概率为<span class="math inline">\(p(\mathcal{C}_k)\)</span>,那么我们的任务就是使用贝叶斯公式计算后验概 率<span class="math inline">\(p(\mathcal{C}_k|\mathbf{x})\)</span>.由于我们只考虑两类的情况，对于类<span class="math inline">\(\mathcal{C}_1\)</span>的后验概率可以写为</p>
<p><span class="math display">\[
\begin{aligned}
p(\mathcal{C}_1|\mathbf{x}) &amp;= \frac{p(\mathbf{x}|\mathcal{C}_1) p(\mathcal{C}_1)}
{p(\mathbf{x}|\mathcal{C}_1) p(\mathcal{C}_1) + p(\mathbf{x}|\mathcal{C}_2) p(\mathcal{C}_2)} \\
    &amp;= \frac{1}{1 + \exp(-a)} = \sigma(a)
\end{aligned}
\]</span></p>
<p>其中<span class="math inline">\(a\)</span>是一个非常关键的参数，整个模型都由这个参数来决定，定义为</p>
<p><span class="math display">\[ a = \ln{\frac{p(\mathbf{x}|\mathcal{C}_1) p(\mathcal{C}_1)}{p(\mathbf{x}|\mathcal{C}_2) p(\mathcal{C}_2)}} \]</span></p>
<p><span class="math inline">\(\sigma(a)\)</span>就是 logistic sigmoid 函数，定义为<span class="math inline">\(\sigma(a) = \frac{1}{1 + \exp(-a)}\)</span>很容易验证 logistic sigmoid 函数的逆为<span class="math inline">\(a = \ln(\frac{\sigma}{1-\sigma})\)</span> 这其实是一个<a href="https://en.wikipedia.org/wiki/Logit" target="_blank" rel="external">logit function</a>,表示两个类概率比率的对数<span class="math inline">\(\ln[p(\mathcal{C}_1|\mathbf{x})/p(\mathcal{C}_2|\mathbf{x})]\)</span> 对于多个类，也很容易得到每一个类的后验概率</p>
<p><span class="math display">\[
\begin{aligned}
p(\mathcal{C}_k | \mathbf{x}) &amp;= \frac{p(\mathbf{x}|\mathcal{C}_k) p(\mathcal{C}_k)}
{\sum_j{p(\mathbf{x}|\mathcal{C}_j) p(\mathcal{C}_j)}} \\
    &amp;= \frac{\exp(a_k)}{\sum_j{\exp(a_j)}}
\end{aligned}
\]</span></p>
<p>其中<span class="math inline">\(a_k\)</span>为</p>
<p><span class="math display">\[ a_k = \ln p(\mathbf{x}|\mathcal{C}_k) p(\mathcal{C}_k) \]</span></p>
<p>其实这个函数也就是 softmax 函数。</p>
<p>从上面的分析看，对于两类的情况(多类也同理)，每个类的后验概率就是 logistic sigmoid 函数，单<span class="math inline">\(a\)</span>是线性的时候，也就是<span class="math inline">\(a = \ln{\frac{p(\mathbf{x}|\mathcal{C}_1) p(\mathcal{C}_1)}{p(\mathbf{x}|\mathcal{C}_2) p(\mathcal{C}_2)}}\)</span>是线性时，我们便可以使用 logistic regression 来对数据集做回归分析，这样就能得到很好的效果。这样说可能还有点模糊，因为我们很难知道一个分布中<span class="math inline">\(a = \ln{\frac{p(\mathbf{x}|\mathcal{C}_1) p(\mathcal{C}_1)}{p(\mathbf{x}|\mathcal{C}_2) p(\mathcal{C}_2)}}\)</span>是不是线性的，其实只要是<a href="https://en.wikipedia.org/wiki/Exponential_family" target="_blank" rel="external">指数族</a>分布就可以用 logistic regression 来做回归，这里就不解释原因了，具体可以参考 Christopher M. Bishop 的<strong>Pattern Recognition And Machine Learning</strong></p>
<p>sigmoid 激活函数如下图</p>
<div align="center" style="margin-bottom:20px">
<img src="/2016/05/22/logistic-regression/sigmoid.png" alt="sigmoid.png" title="">
</div>
<h2 id="代码实现">代码实现</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /usr/bin python</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cPickle <span class="keyword">as</span> pkl</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'wangzx'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim, n_class=<span class="number">2</span>)</span>:</span></span><br><span class="line">        self.n_class = n_class</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.biases = np.random.randn(n_class, <span class="number">1</span>)</span><br><span class="line">        self.weights = np.random.randn(n_class, dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        fd = sigmoid(np.dot(self.weights, x)+self.biases)</span><br><span class="line">        <span class="keyword">return</span> fd / np.sum(fd)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span><br><span class="line">            test_data=None)</span>:</span></span><br><span class="line">        <span class="string">"""使用 mini-batch stochastic gradient descent 方法训练网络。</span><br><span class="line">        ``training_data``是一个列表元组``（x,y）``,代表训练的输入和目标输出。</span><br><span class="line">        如果提供``test_data``的话，在每一个 epoch 完成之后会用模型计算这个数据相对应的</span><br><span class="line">        输出。这个可以让我们更清楚整个训练情况，不过会降低训练的速度。</span><br><span class="line">        """</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                print(<span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                    j, self.evaluate(test_data), n_test))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">"Epoch &#123;0&#125; complete"</span>.format(j))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">        <span class="string">"""通过随机梯度下降的方法来更新权值。</span><br><span class="line">        ``mini_batch``是一个元组列表``(x,y)``,每一项是一个训练数据，分别代表</span><br><span class="line">        训练输入和目标输出，``eta``是学习率"""</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># nabla_b 和 nabla_w 分别表示 b 和 w 的梯度</span></span><br><span class="line">        nabla_b = np.zeros_like(self.biases)</span><br><span class="line">        nabla_w = np.zeros_like(self.weights)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b += delta_nabla_b</span><br><span class="line">            nabla_w += delta_nabla_w</span><br><span class="line"></span><br><span class="line">        self.weights -= eta / len(mini_batch) * nabla_w</span><br><span class="line">        self.biases -= eta / len(mini_batch) * nabla_b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""其实可以不用写一个 backprop 函数，直接算梯度就行了，不过为了和 bp 神经网络</span><br><span class="line">        的代码结构尽量保持相似，所以也写了一个 backprop 函数"""</span></span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = self.feedforward(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activation, y) * \</span><br><span class="line">            sigmoid_prime(np.dot(self.weights, x) + self.biases)</span><br><span class="line">        <span class="keyword">return</span> delta, np.dot(delta, x.T)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, test_data)</span>:</span></span><br><span class="line">        <span class="string">"""返回预测准确的样本个数"""</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">        <span class="comment"># 因为 y 是一个常数值，要转为 one-hot 编码</span></span><br><span class="line">        t = np.zeros((self.n_class, <span class="number">1</span>))</span><br><span class="line">        t[y, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#使用欧氏距离损失函数，其导数如下</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""sigmoid 函数."""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""sigmoid 函数的导数."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span></span><br><span class="line">    feat = pkl.load(open(<span class="string">"data/train_X.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    <span class="comment">#对数据进行归一化处理</span></span><br><span class="line">    feat = (feat - np.min(feat, axis=<span class="number">0</span>)) / (np.max(feat, axis=<span class="number">0</span>) - np.min(feat, axis=<span class="number">0</span>) + <span class="number">1e-3</span>)</span><br><span class="line">    y = pkl.load(open(<span class="string">"data/train_Y.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    train_data = [(np.asarray(f).reshape((<span class="number">-1</span>,<span class="number">1</span>)), t) <span class="keyword">for</span> f, t <span class="keyword">in</span> zip(feat, y)]</span><br><span class="line">    test_feat = pkl.load(open(<span class="string">"data/test_X.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    test_feat = (test_feat - np.min(test_feat, axis=<span class="number">0</span>)) / \</span><br><span class="line">                (np.max(test_feat, axis=<span class="number">0</span>) - np.min(test_feat, axis=<span class="number">0</span>) + <span class="number">1e-3</span>)</span><br><span class="line">    test_y = pkl.load(open(<span class="string">"data/test_Y.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    test_data = [(np.asarray(f).reshape((<span class="number">-1</span>,<span class="number">1</span>)), t) <span class="keyword">for</span> f, t <span class="keyword">in</span> zip(test_feat, test_y)]</span><br><span class="line">    n_sample, ndim = feat.shape</span><br><span class="line">    logistic = LogisticRegression(ndim)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Begin fit training data"</span>)</span><br><span class="line">    logistic.SGD(train_data, <span class="number">50</span>, <span class="number">50</span>, <span class="number">0.01</span>, test_data)</span><br><span class="line"></span><br><span class="line">    score = logistic.evaluate(test_data) / len(test_data)</span><br><span class="line">    print(<span class="string">"Test accuracy is %f"</span> % score)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_mnist</span><span class="params">()</span>:</span></span><br><span class="line">    path = <span class="string">'/home/wangzx/usr/share/data/mnist2/'</span></span><br><span class="line">    train_X = pkl.load(open(path + <span class="string">'mnist_train_X.pkl'</span>))</span><br><span class="line">    train_y = pkl.load(open(path + <span class="string">'mnist_train_y.pkl'</span>)).ravel()</span><br><span class="line">    test_X = pkl.load(open(path + <span class="string">'mnist_test_X.pkl'</span>))</span><br><span class="line">    test_y = pkl.load(open(path + <span class="string">'mnist_test_y.pkl'</span>)).ravel()</span><br><span class="line">    train_data = [(np.asarray(f).reshape((<span class="number">-1</span>,<span class="number">1</span>)), t) <span class="keyword">for</span> f, t <span class="keyword">in</span> zip(train_X, train_y)]</span><br><span class="line">    test_data = [(np.asarray(f).reshape((<span class="number">-1</span>,<span class="number">1</span>)), t) <span class="keyword">for</span> f, t <span class="keyword">in</span> zip(test_X, test_y)]</span><br><span class="line">    n_sample, ndim = train_X.shape</span><br><span class="line">    logistic = LogisticRegression(ndim, n_class=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Begin fit training data"</span>)</span><br><span class="line">    logistic.SGD(train_data, <span class="number">50</span>, <span class="number">50</span>, <span class="number">0.02</span>, test_data)</span><br><span class="line"></span><br><span class="line">    score = logistic.evaluate(test_data) / len(test_data)</span><br><span class="line">    print(<span class="string">"Test accuracy is %f"</span> % score)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#test()</span></span><br><span class="line">    <span class="comment">#test_mnist()</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[K means Clustering]]></title>
      <url>http://shuimuth.github.io/2016/05/22/kmeans/</url>
      <content type="html"><![CDATA[<h1 id="k-means-clustering">K-means Clustering</h1>
<p>k-means 算法源于信号处理中的一种矢量量化方法，现在则更多地作为一种聚类分析方法流行于数据挖掘领域。k-means 聚类的目的是：把 n 个点（可以是样本的一次观察或一个实例）划分到 k 个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准。详细介绍可以查看<a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank" rel="external">维基百科</a>的解释 <a id="more"></a></p>
<h2 id="核心算法">核心算法</h2>
<p>已知观测集<span class="math inline">\((x_{1},x_{2},...,x_{n})\)</span>，其中每个观测都是一个 d-维实矢量，k-means 聚类要把这 n 个观测划分到 k 个集合中(k≤n),使得组内平方和最小。换句话说，它的目标是找到使得满足下式 <span class="math display">\[\arg\min_S \sum_{i=1}^{k}{ \sum_{x\in S_i}{\|x_i - \mu_i \|^2} }\]</span></p>
<p>其中<span class="math inline">\(\mu_i\)</span>是<span class="math inline">\(S_i\)</span>中所有点的平均值</p>
<h2 id="算法流程描述">算法流程描述</h2>
<p>下面用伪代码来描述下算法的具体过程</p>
<ul>
<li>初始化 均值 <span class="math inline">\(m_1, m_2, \ldots , m_k\)</span></li>
<li>until 均值不再改变
<ul>
<li>使用估计的均值根据距离最近的原则把各个样本分到相应的聚类当中</li>
<li>For i from 1 to k
<ul>
<li>用聚类<span class="math inline">\(i\)</span>中所有样本的均值来代替<span class="math inline">\(m_i\)</span></li>
</ul></li>
<li>end_for</li>
</ul></li>
<li>end_until</li>
</ul>
<h2 id="代码实现">代码实现</h2>
<p>下面看看 k-mean 算法具体是如何实现的，下面这段代码是生成数据的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">X_mean = <span class="number">8</span> + (<span class="number">4</span> * np.random.rand(<span class="number">4</span>, <span class="number">2</span>))  <span class="comment"># N(8,4)</span></span><br><span class="line">which = np.random.choice(np.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]), size=<span class="number">100</span>, replace=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, X.shape[<span class="number">0</span>]):</span><br><span class="line">    X[i] = X[i] + X_mean[which[i], :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the points</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.scatter(X[which == <span class="number">0</span>][:, <span class="number">0</span>], X[which == <span class="number">0</span>][:, <span class="number">1</span>], c=<span class="string">'blue'</span>)</span><br><span class="line">ax.scatter(X[which == <span class="number">1</span>][:, <span class="number">0</span>], X[which == <span class="number">1</span>][:, <span class="number">1</span>], c=<span class="string">'green'</span>)</span><br><span class="line">ax.scatter(X[which == <span class="number">2</span>][:, <span class="number">0</span>], X[which == <span class="number">2</span>][:, <span class="number">1</span>], c=<span class="string">'red'</span>)</span><br><span class="line">ax.scatter(X[which == <span class="number">3</span>][:, <span class="number">0</span>], X[which == <span class="number">3</span>][:, <span class="number">1</span>], c=<span class="string">'cyan'</span>)</span><br></pre></td></tr></table></figure>
<p>生成的数据如下图</p>
<div align="center">
<img src="/2016/05/22/kmeans/data.png" alt="数据" title="数据">
</div>
<p>接下来的这段代码是 K 均值的核心部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">4</span>, n_init=<span class="number">15</span>)</span><br><span class="line">kmeans.fit(X)</span><br><span class="line">ypred = kmeans.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print confusion matrix. Note that the matrix is not aligned because we don't know</span></span><br><span class="line"><span class="comment"># the correspondence between the assigned cluster and the generated cluster, but the</span></span><br><span class="line"><span class="comment"># matrix should show one high value per row and/or column.</span></span><br><span class="line">confusion_matrix = np.zeros((<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, which.shape[<span class="number">0</span>]):</span><br><span class="line">    actual = which[i]</span><br><span class="line">    predicted = ypred[i]</span><br><span class="line">    confusion_matrix[actual, predicted] = confusion_matrix[actual, predicted] + <span class="number">1</span></span><br><span class="line"><span class="keyword">print</span> confusion_matrix</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot points with cluster centers (marked with +)</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.scatter(X[which == <span class="number">0</span>][:, <span class="number">0</span>], X[which == <span class="number">0</span>][:, <span class="number">1</span>], c=<span class="string">'blue'</span>)</span><br><span class="line">ax.scatter(X[which == <span class="number">1</span>][:, <span class="number">0</span>], X[which == <span class="number">1</span>][:, <span class="number">1</span>], c=<span class="string">'green'</span>)</span><br><span class="line">ax.scatter(X[which == <span class="number">2</span>][:, <span class="number">0</span>], X[which == <span class="number">2</span>][:, <span class="number">1</span>], c=<span class="string">'red'</span>)</span><br><span class="line">ax.scatter(X[which == <span class="number">3</span>][:, <span class="number">0</span>], X[which == <span class="number">3</span>][:, <span class="number">1</span>], c=<span class="string">'cyan'</span>)</span><br><span class="line"><span class="keyword">for</span> cc <span class="keyword">in</span> kmeans.cluster_centers_:</span><br><span class="line">    ax.plot(cc[<span class="number">0</span>], cc[<span class="number">1</span>], marker=<span class="string">'+'</span>, color=<span class="string">'black'</span>, markersize=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<div align="center">
<img src="/2016/05/22/kmeans/class.png" alt="分类" title="分类">
</div>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[人脸识别]]></title>
      <url>http://shuimuth.github.io/2016/05/22/face/</url>
      <content type="html"><![CDATA[<p>最近无人机很火爆，使用无人机可以到处装逼，看着的确高大尚。不能放过这个装逼机会。 近期与同学做了一个基于四轴飞 行器的人脸追踪项目，简单来说就是通过安装在云台上的摄像机获取 图像，然后使用人脸检测与识别算法在图像中定位目标 人物所在位置，有了位置的反馈信息就能够 自动控制飞行器的飞行了。在这个项目中我主要负责的是人脸识别的部分，飞行 器控制部分则由另外 一个组员负责，我们分工很明确，各有所长，绝佳搭配啊。 <a id="more"></a></p>
<p>这次主要完成人脸检测与人脸识别两个任务，人脸检测使用的是基于 haar-like 特征的级联检测算法， 人脸识别则使用 eigen face、fisher face 和 LBPH 算法。下面介绍下各个算法的细节。</p>
<h1 id="人脸检测haar-like-特征">人脸检测：haar-like 特征</h1>
<p>人脸检测简单来说就是在一幅图像中确定人脸所在位置（如果有的话），换句话来说其实也就是给定一个 区域，判断该区域是人脸还是背景。所以人脸检测也是个 binary classification 问题。 这样一来问题就变得比较清晰了，要做分类问题必须要获得描述对象的特征，好的特征对于分类任务是 极其重要的。好在这些都有前人帮我们铺好路了，Paul Viola and Michael Jones 2001 年在他们的论文 <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=990517" target="_blank" rel="external"><em>Rapid Object Detection using a Boosted Cascade of Simple Features</em></a> 中提出了一种 基于 haar-like 特征的高效物体检测算法，提出的 haar 特征如下图。之后 Rainer Lienhart 和 Jochen Maydt 对这一 特征进行了拓展，得到性能更好的分类器，拓展 <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1038171" target="_blank" rel="external"><em>haar-like 特征</em></a> 。Haar 特征和卷积核很像，每 个特征值都是白色矩形框内的像素 之和减去黑色矩形框内像素之和。具体可参考 <a href="http://docs.opencv.org/3.1.0/d7/d8b/tutorial_py_face_detection.html#gsc.tab=0" target="_blank" rel="external"><em>opencv 官方教程</em></a> 。</p>
<div align="center" style="margin-bottom:20px">
<img src="/2016/05/22/face/haarori.png" alt="haar-like 特征" title="haar-like 特征">
</div>
<h1 id="人脸识别lbph">人脸识别：LBPH</h1>
<p>人脸识别也是一个分类任务，给定一幅人脸图像，判断该图像中人物是谁。那么我们该如何用数 学语言来描述人脸呢？也就是人脸图像的特征向量如何计算得到，这次项目使用的是 LBP 特征。最初 LBP 特征 是 Ojala 等人在论文 <a href="http://www.sciencedirect.com/science/article/pii/0031320395000674" target="_blank" rel="external"><em>A comparative study of texture measures with classification based on featured distributions</em></a> 中提出的，这种特征对于纹理的描述非常有效，其显著优点是对关照不敏感。 LBP 是定义在像素 3x3 邻域内的，以邻域中心像素为阈值，将相邻的 8 个像素的灰度值与其进行比较， 若周围像素值大于中心像素值，则该像素点的位置被标记为 1，否则为 0。这样，3x3 邻域内的 8 个点经比较可产生 8 位二进制数（通常转换为十进制数即 LBP 码，共 256 种），即得到该邻域中心像素点的 LBP 值， 并用这个值来反映该区域的纹理信息。具体如下图所示：</p>
<div align="center" style="margin-bottom:20px">
<img src="/2016/05/22/face/lbpop.png" alt="基本 LBP 操作" title="基本 LBP 操作">
</div>
<p>更加形式化的描述如下：</p>
\begin{equation}
    LBP(x_c,y_c) = \sum_{p=0}^{p-1}2^ps(i_p,i_c)
\end{equation}
<p>其中<span class="math inline">\((x_c,y_c)\)</span>代表 <span class="math inline">\(3\times3\)</span> 邻域的中心元素,像素值为<span class="math inline">\(i_c\)</span>,邻域的其他像素值为 <span class="math inline">\(i_p\)</span>，<span class="math inline">\(s(x)\)</span>是符号 函数，定义如下：</p>
基本的 LBP 算子的最大缺陷在于它只覆盖了一个固定半径范围内的小区域，这显然不能满足不同尺寸和频率纹理的需要。 为了适应不同尺度的纹理特征，并达到灰度和旋转不变性的要求，Ahonen 等在论文 <a href="http://uran.donetsk.ua/~masters/2011/frt/dyrul/library/article8.pdf" target="_blank" rel="external"><em>Face recognition with local binary patterns</em></a> 中对 LBP 算子进行了改进，将 <span class="math inline">\(3\times3\)</span> 邻域扩展到任意邻 域，并用圆形邻域代替了正方形邻域，改进后的 LBP 算子允许在半径为 R 的圆形邻域内有任意多个像素点。 从而得到了诸如半径为 R 的圆形区域内含有 P 个采样点的 LBP 算子。 对于一个给定邻域中心点<span class="math inline">\((x_c,y_c)\)</span>，其邻域采样点的位置通过以下公式计算得到：
\begin{eqnarray}
  x_p = x_c + R \cos(\frac{2\pi p} {P})\\
  y_p = y_c + R \sin(\frac{2\pi p}{P})
\end{eqnarray}
<p>其中<span class="math inline">\(R\)</span>是圆半径，<span class="math inline">\(P\)</span>为采样点数，<span class="math inline">\(p \in P\)</span>。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[K Nearest Neighborhood]]></title>
      <url>http://shuimuth.github.io/2016/05/22/2016:05:22:knn/</url>
      <content type="html"><![CDATA[<h1 id="k-nearest-neighborhood">K Nearest Neighborhood</h1>
<p>下面引用下维基百科对 KNN 的描述，具体点击<a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" target="_blank" rel="external">这里</a> <blockquote><p>在模式识别领域中，最近邻居法（KNN 算法，又译 K-近邻算法）是一种用于分类和回归的非参数统计方法。在这两种情况下，输入包含特征空间中的 k 个最接近的训练样本。</p>
<ul>
<li>在 k-NN 分类中，输出是一个分类族群。一个对象的分类是由其邻居的“多数表决”确定的，k 个最近邻居（k 为正整数，通常 较小）中最常见的分类决定了赋予该对象的类别。若 k = 1，则该对象的类别直接由最近的一个节点赋予。</li>
<li>在 k-NN 回归中，输出是该对象的属性值。该值是其 k 个最近邻居的值的平均值。</li>
</ul>
</blockquote> <a id="more"></a></p>
<h2 id="算法">算法</h2>
<p>训练样本是多维特征空间向量，其中每个训练样本带有一个类别标签。算法的训练阶段只包含存储的特征向量和训练样本的标签。 在分类阶段，k 是一个用户定义的常数。一个没有类别标签的向量（查询或测试点）将被归类为最接近该点的 k 个样本点中最频 繁使用的一类。如何定义两个样本点在 KNN 中是最关键的，根据不同的数据分布特征可以选择不同的距离公式，最常用的是欧 氏距离、汉明距离、余弦距离，也有使用 Pearson 和 Spearman 相关系数的。如果适当运用算法来计算相似度(距离)的话，k 近邻 分类精度可显著提高。无论是分类还是回归，衡量邻居的权重都非常有用，使较近邻居的权重比较远邻居的权重大。例如，一 种常见的加权方案是给每个邻居权重赋值为 <span class="math inline">\(\frac{1}{d}\)</span>，其中<span class="math inline">\(d\)</span>是到邻居的距离</p>
<h2 id="特点">特点</h2>
<p>KNN 使用“多数表决”进行分类，这样会在类别分布偏斜时出现缺陷。也就是说，出现频率较多的样本将会主导测试点的预测结 果，因为他们比较大可能出现在测试点的 K 邻域而测试点的属性又是通过 k 邻域内的样本计算出来的。 <blockquote><p>解决这个缺点的方法之一是在进行分类时将样本到 k 个近邻点的距离考虑进去。k 近邻点中每一个的分类（对于回归问题来说，是数值）都乘以与测试点之间距离的成反比的权重。另一种克服偏斜的方式是通过数据表示形式的抽象</p>
</blockquote></p>
<h2 id="决策过程">决策过程</h2>
具体过程可以参考维基百科这幅图
<div align="center">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/279px-KnnClassification.svg.png">
</div>
<p>测试样本（绿色圆形）应归入要么是第一类的蓝色方形或是第二类的红色三角形。如果 k=3（实线圆圈）它被分配给第二类， 因为有 2 个三角形和只有 1 个正方形在内侧圆圈之内。如果 k=5（虚线圆圈）它被分配到第一类（3 个正方形与 2 个三角形在外侧 圆圈之内）。</p>
<h2 id="python-代码实现">python 代码实现</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kNN</span><span class="params">(test, train, target, k=<span class="number">1</span>, normalize=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    This is the k nearest neighbour algorithm</span><br><span class="line">    :param test: N*D numpy array, each row record an example,</span><br><span class="line">    each column represent a feature field</span><br><span class="line">    :param train: N*D numpy array, each row record an example,</span><br><span class="line">    each column represent a feature field</span><br><span class="line">    :param target: 1*D numpy array, each element is the target value</span><br><span class="line">    :param k: int, the best k near neighbour</span><br><span class="line">    :param normalize: bool, determine whether to normalize `test` and `train` or not</span><br><span class="line">    :return: 1*D numpy array, the predict value</span><br><span class="line">    """</span></span><br><span class="line">    y_hat = np.zeros(test.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> normalize:</span><br><span class="line">        test = test / norm(test, axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">        train = train / norm(train, axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(test.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="comment">#tmp = train - test[i]</span></span><br><span class="line">        dist = distance(train, test[i], method=<span class="string">"cosine"</span>)</span><br><span class="line">        idx = dist.argsort()[:k]</span><br><span class="line">        <span class="comment"># given the weight of each neighbour</span></span><br><span class="line">        w = np.array([<span class="number">1</span>]) / dist[idx]</span><br><span class="line">        w /= w.sum()</span><br><span class="line">        y_hat[i] = (target[idx] * w).sum()</span><br><span class="line">    <span class="keyword">return</span> y_hat</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">(x, y, method=<span class="string">"euclidean"</span>, axis=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    compute the distance between `x` and `y` by `method`</span><br><span class="line">    :param x: 1*D numpy array or array like</span><br><span class="line">    :param y: 1*D numpy array or array like</span><br><span class="line">    :param method: the method using to compute the distance between `x` and `y`</span><br><span class="line">                    the method can only be "euclidean" or "cosine"</span><br><span class="line">    :return: a float number represent the distance between `x` and `y`</span><br><span class="line">    """</span></span><br><span class="line">    x = np.asarray(x)</span><br><span class="line">    y = np.asarray(y)</span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">"cosine"</span>:</span><br><span class="line">        tmp = (x * y).sum(axis=axis) / (np.linalg.norm(x, axis=axis) * np.linalg.norm(y))</span><br><span class="line">        <span class="keyword">return</span> np.array([<span class="number">1</span>]) / (tmp + <span class="number">1e-3</span>)</span><br><span class="line">    <span class="keyword">elif</span> method == <span class="string">"euclidean"</span>:</span><br><span class="line">        <span class="keyword">return</span> np.linalg.norm(x - y, axis=axis)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> <span class="built_in">NotImplemented</span>(<span class="string">"Not implement method"</span> + method)</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Back Propagation Neural Network]]></title>
      <url>http://shuimuth.github.io/2016/05/22/bp/</url>
      <content type="html"><![CDATA[<h1 id="back-propagation-neural-network">Back Propagation Neural Network</h1>
<h2 id="模型架构">模型架构</h2>
BP 神经网络是一种多层的前馈神经网络，其主要的特点是：信号是前向传播的，而误差是反向传播的。BP 神经网络的过程主要 分为两个阶段，第一阶段是信号的前向传播，从输入层经过隐含层，最后到达输出层；第二阶段是误差的反向传播，从输出层 到隐含层，最后到输入层，依次调节隐含层到输出层的权重和偏置，输入层到隐含层的权重和偏置。其架构如图所示 <a id="more"></a>
<div align="center" style="margin-bottom:20px">
<img src="/2016/05/22/bp/bp.png" alt="BP 神经网络" title="BP 神经网络">
</div>
<h3 id="前向传播">前向传播</h3>
<p>在训练网络之前，我们需要随机初始化权重和偏置，对每一个权重取[-1,1]的一个随机实数，每一个偏置取[0,1]的一个随机实数，之后就开始进行前向传播。前向传播比较简单，只要按照模型的方法一层一层的计算即可，具体公式如下</p>
<p><span class="math display">\[I_j = \sum_i{w_{ij}O_i} + b_j  \qquad O_j = \frac{1}{1+\exp^{-I_j}}\]</span></p>
<p>其中<span class="math inline">\(I_j\)</span>是第<span class="math inline">\(j\)</span>层网络的输入，<span class="math inline">\(O_j\)</span>是第<span class="math inline">\(j\)</span>层网络的输出，<span class="math inline">\(w_{ij}\)</span>是第<span class="math inline">\(i\)</span>层到第<span class="math inline">\(j\)</span>层的连接权重</p>
<h3 id="后向传播">后向传播</h3>
<p>后向传播从最后一层即输出层开始，我们训练神经网络作分类的目的往往是希望最后一层的输出能够描述数据记录的类别，比如对于一个二分类的问题，我们常常用两个神经单元作为输出层，如果输出层的第一个神经单元的输出值比第二个神经单元大，我们认为这个数据记录属于第一类，否则属于第二类。对于一个随机初始化参数的网络是不能够很好地描述数据集，因此需要对参数进行调整。这个调整过程就是通过误差的后向传播来完成的。误差<span class="math inline">\(e = |\mathbf{o} - \mathbf{y}|_2\)</span>,其中<span class="math inline">\(\mathbf{o}\)</span>是输出向量,<span class="math inline">\(\mathbf{y}\)</span>是目标向量。因此在数据集<span class="math inline">\(\mathcal{D}\)</span>中，损失函数<span class="math inline">\(\mathcal{J}\)</span>可定义为</p>
<p><span class="math display">\[\mathcal{J}(w,b) = \frac{1}{2}\sum_{i=1}^{|\mathcal{D}|}e_i\]</span></p>
<p>其中<span class="math inline">\(|\mathcal{D}|\)</span>表示数据集<span class="math inline">\(\mathcal{D}\)</span>的样本总数，<span class="math inline">\(e_i\)</span>表示第<span class="math inline">\(i\)</span>个样本的误差值。 通过权值更新公式 <span class="math inline">\((w,b) := (w,b) - \alpha(\nabla_{(w,b)}\mathcal{J}(w,b))\)</span> 更新权值即可。具体细节的推导比较复杂，所以只写到这一步，通过阅读代码也能很好地理解后向传播的具体过程。 这里给出我觉得不错的教程</p>
<ul>
<li><a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">How the backpropagation algorithm works</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/Neural_Networks" target="_blank" rel="external">Neural Networks</a></li>
</ul>
<h3 id="训练终止条件">训练终止条件</h3>
<p>每一轮训练都使用数据集的所有记录，但什么时候停止，停止条件有下面两种：</p>
<ol style="list-style-type: decimal">
<li>设置最大迭代次数，比如使用数据集迭代 100 次后停止训练</li>
<li>计算训练集在网络上的预测准确率，达到一定门限值后停止训练</li>
</ol>
<p>这次算法使用的是第一种方法，后续如果时间充足的话再去实现第二种。</p>
<h2 id="代码实现">代码实现</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /usr/bin python</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cPickle <span class="keyword">as</span> pkl</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'wangzx'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        <span class="string">"""``sizes``是 list 类型，第 i 个值表示第 i 层所包含的神经元个数。</span><br><span class="line">        例如，如果 sizes=[2,3,1],那么就表示一个三层网络，第一层有 2 个神经元，</span><br><span class="line">        第二层有 3 个神经元，最后一层有 1 个神经元。网络的偏置 b 和权值 w 使用标准高斯</span><br><span class="line">        分布来初始化。需要注意的一点是网络的第一层表示输入层，最后一层表示输出层</span><br><span class="line">        """</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a / np.sum(a)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span><br><span class="line">            test_data=None)</span>:</span></span><br><span class="line">        <span class="string">"""使用 mini-batch stochastic gradient descent 方法训练网络。</span><br><span class="line">        ``training_data``是一个列表元组``（x,y）``,代表训练的输入和目标输出。</span><br><span class="line">        如果提供``test_data``的话，在每一个 epoch 完成之后会用模型计算这个数据相对应的</span><br><span class="line">        输出。这个可以让我们更清楚整个训练情况，不过会降低训练的速度。</span><br><span class="line">        """</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                print(<span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                    j, self.evaluate(test_data), n_test))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">"Epoch &#123;0&#125; complete"</span>.format(j))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">        <span class="string">"""通过随机梯度下降的方法来更新权值。</span><br><span class="line">        ``mini_batch``是一个元组列表``(x,y)``,每一项是一个训练数据，分别代表</span><br><span class="line">        训练输入和目标输出，``eta``是学习率"""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/len(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""返回损失函数 C_x 的梯度(nabla_b, nabla_w). ``nabla_b``表示偏置 b 的梯度，``nabla_w``表示</span><br><span class="line">        权重 w 的梯度。需要注意一点，返回的是每一层的梯度，也就是说返回值是一个 list，list 的每一项代表</span><br><span class="line">        网络其中一层的梯度"""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下面这一部分可能有点晦涩，这里使用了 python 的索引特性：允许使用负数进行索引</span></span><br><span class="line">        <span class="comment"># 因为最后一层是输出层，作为后向传播的初始阶段，在上面做了初始化，所以从倒数</span></span><br><span class="line">        <span class="comment"># 第二层开始</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, test_data)</span>:</span></span><br><span class="line">        <span class="string">"""返回预测准确的样本个数"""</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">        <span class="comment"># 因为 y 是一个常数值，要转为 one-hot 编码</span></span><br><span class="line">        t = np.zeros((self.sizes[<span class="number">-1</span>], <span class="number">1</span>))</span><br><span class="line">        t[y, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="comment">#print("output activation is &#123;0&#125;".format(output_activations))</span></span><br><span class="line">        <span class="comment">#print("y is &#123;0&#125;".format(t))</span></span><br><span class="line">        <span class="comment"># 使用最小二乘估计，损失函数导数如下</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-t)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""sigmoid 函数."""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""sigmoid 函数的导数."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span></span><br><span class="line">    feat = pkl.load(open(<span class="string">"data/train_X.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    feat = (feat - np.min(feat, axis=<span class="number">0</span>)) / \</span><br><span class="line">                (np.max(feat, axis=<span class="number">0</span>) - np.min(feat, axis=<span class="number">0</span>) + <span class="number">1e-3</span>)</span><br><span class="line">    y = pkl.load(open(<span class="string">"data/train_Y.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    train_data = [(np.asarray(f).reshape((<span class="number">-1</span>,<span class="number">1</span>)), t) <span class="keyword">for</span> f, t <span class="keyword">in</span> zip(feat, y)]</span><br><span class="line">    test_feat = pkl.load(open(<span class="string">"data/test_X.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    test_feat = (test_feat - np.min(test_feat, axis=<span class="number">0</span>)) / \</span><br><span class="line">                (np.max(test_feat, axis=<span class="number">0</span>) - np.min(test_feat, axis=<span class="number">0</span>) + <span class="number">1e-3</span>)</span><br><span class="line">    test_y = pkl.load(open(<span class="string">"data/test_Y.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    test_data = [(np.asarray(f).reshape((<span class="number">-1</span>,<span class="number">1</span>)), t) <span class="keyword">for</span> f, t <span class="keyword">in</span> zip(test_feat, test_y)]</span><br><span class="line">    n_sample, ndim = feat.shape</span><br><span class="line">    bp = Network([ndim, <span class="number">50</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Begin fit training data"</span>)</span><br><span class="line">    bp.SGD(train_data, <span class="number">50</span>, <span class="number">50</span>, <span class="number">0.02</span>, test_data)</span><br><span class="line"></span><br><span class="line">    score = bp.evaluate(test_data) / len(test_data)</span><br><span class="line">    print(<span class="string">"Test accuracy is %f"</span> % score)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_mnist</span><span class="params">()</span>:</span></span><br><span class="line">    path = <span class="string">'/home/wangzx/usr/share/data/mnist2/'</span></span><br><span class="line">    train_X = pkl.load(open(path + <span class="string">'mnist_train_X.pkl'</span>))</span><br><span class="line">    train_y = pkl.load(open(path + <span class="string">'mnist_train_y.pkl'</span>)).ravel()</span><br><span class="line">    test_X = pkl.load(open(path + <span class="string">'mnist_test_X.pkl'</span>))</span><br><span class="line">    test_y = pkl.load(open(path + <span class="string">'mnist_test_y.pkl'</span>)).ravel()</span><br><span class="line">    train_data = [(np.asarray(f).reshape((<span class="number">-1</span>,<span class="number">1</span>)), t) <span class="keyword">for</span> f, t <span class="keyword">in</span> zip(train_X, train_y)]</span><br><span class="line">    test_data = [(np.asarray(f).reshape((<span class="number">-1</span>,<span class="number">1</span>)), t) <span class="keyword">for</span> f, t <span class="keyword">in</span> zip(test_X, test_y)]</span><br><span class="line">    n_sample, ndim = train_X.shape</span><br><span class="line">    bp = Network([ndim, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Begin fit training data"</span>)</span><br><span class="line">    bp.SGD(train_data, <span class="number">50</span>, <span class="number">50</span>, <span class="number">0.1</span>, test_data)</span><br><span class="line"></span><br><span class="line">    score = bp.evaluate(test_data) / len(test_data)</span><br><span class="line">    print(<span class="string">"Test accuracy is %f"</span> % score)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#test()</span></span><br><span class="line">    <span class="comment">#test_mnist()</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Decision Tree]]></title>
      <url>http://shuimuth.github.io/2016/05/22/decision-tree/</url>
      <content type="html"><![CDATA[<h1 id="decision-tree">Decision Tree</h1>
<h2 id="决策树模型">决策树模型</h2>
<p>下面引用下维基百科对决策树的描述:</p>
<blockquote>
<p>机器学习中，决策树是 一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，而每个叶结点则对应 从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。 数据挖掘中决策树是一种经常要用到的技术，可以用于分析数据，同样也可以用来作预测。</p>
</blockquote>
<a id="more"></a>
<h2 id="决策树分类原理">决策树分类原理</h2>
<p>决策树是一个非常常用的机器学习算法，其中的一个重要的特点是<strong>简单</strong>，使用者基本上不用了解机器学习算法，也不用深 究它是如何工作的。直观看上去，决策树分类器就像判断模块和终止块组成的流程图，终止块表示分类结果（也就是树的叶 子）。判断模块表示对一个特征取值的判断（该特征有几个值，判断模块就有几个分支），非常简单直观。</p>
<p>使用决策树进行分类时，样本所有特征中有一些特征在分类时起到决定性作用，决策树的构造过程就是找到这些具有决定性作 用的特征，根据其决定性程度来构造一个倒立的树–决定性作用最大的那个特征作为根节点，然后递归找到各分支下子数据集 中次大的决定性特征，直至子数据集中所有数据都属于同一类。所以，构造决策树的过程本质上就是根据数据特征将数据集分 类的递归过程，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。</p>
<h2 id="id3-算法">ID3 算法</h2>
<p>ID3 算法的核心思想就是以信息增益度量属性选择，选择分裂后信息增益最大的属性进行分裂。 如果<span class="math inline">\(\mathcal{D}\)</span>为数据集，<span class="math inline">\(d^{(i)}\)</span>表示第<span class="math inline">\(i\)</span>个样本的类标，依此对训练数据进行的划分，则数据集<span class="math inline">\(\mathcal{D}\)</span>中类标<span class="math inline">\(d\)</span>的熵为：</p>
<p><span class="math display">\[H(d;\mathcal{D}) = -\sum_i^m{p(i)\log(p(i))}\]</span></p>
<p>其中<span class="math inline">\(p(i)\)</span>表示第 i 个类别在整个训练数据中出现的概率，<span class="math inline">\(m\)</span>表示<span class="math inline">\(d\)</span>所有可能取值的个数，可以用属于此类别的数量除以训练数据总数量作为估计。 熵的实际上表示是混乱程度的一种度量， 当我们将训练集<span class="math inline">\(\mathcal{D}\)</span>按属性 A 进行划分，则 A 对<span class="math inline">\(\mathcal{D}\)</span>划分后的熵为:</p>
<p><span class="math display">\[H_A(d;\mathcal{D}) = \sum_{j\in A}{\frac{|\mathcal{D}_j|}{\mathcal{|D|}} H(d;\mathcal{D}_j)}\]</span></p>
<p>其中<span class="math inline">\(\mathcal{D}_j\)</span>表示数据集<span class="math inline">\(\mathcal{D}\)</span>中类标为<span class="math inline">\(d(j)\)</span>的组成数据集，<span class="math inline">\(|\mathcal{D}_j|\)</span>表示数据集<span class="math inline">\(\mathcal{D}_j\)</span>的样本数 而信息增益则定义为上述两个熵的差值:</p>
<p><span class="math display">\[\text{gain}(A) = H(d;\mathcal{D}) - H_A(d;\mathcal{D})\]</span></p>
<p>ID3 算法就是在每次需要分裂时，计算每个属性的增益率，然后选择增益率最大的属性进行分裂。</p>
<h3 id="c4.5-算法">C4.5 算法</h3>
<p>ID3 算法存在一个问题，就是偏向于多值属性，例如，如果存在唯一标识属性 ID，则 ID3 会选择它作为分裂属性，这样虽然 使得划分充分纯净，但这种划分对分类可能是不利的。ID3 的后继算法 C4.5 使用增益率的信息增益扩充，试图克服这个缺点。 C4.5 算法用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性，C4.5 定义的分裂信息可以表示成： <span class="math display">\[\text{splitInfo}_A(d;\mathcal{D}) = \sum_{j \in A}{\frac{|\mathcal{D}_j|}{\mathcal{|D|}} \log(\frac{|\mathcal{D}_j|}{\mathcal{|D|}})}\]</span> 然后，信息增益率被定义为： <span class="math display">\[ \text{gainRatio(A)} = \frac{\text{gain(A)} } {\text{splitInfo}_A(d;\mathcal{D})}\]</span> 在选择具有最佳分类效果的属性时，C4.5 算法是根据信息增益率来选择，而不是信息增益。</p>
<p>为了日后更好地理解决策树的构建，在维基百科上找了一张决策树的图，不过就不做具体分析了。</p>
<div style="margin-bottom:20px" align="center">
<img src="https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png" title="决策树">
</div>
<h2 id="决策树优缺点">决策树优缺点</h2>
<p>在网上搜索了关于决策树的优点和不足之处，现总结如下:</p>
<ul>
<li>决策树适用于数值型和标称型（离散型数据，变量的结果只在有限目标集中取值），能够读取数据集合，提取一些列数据中蕴含的规则。</li>
<li>在分类问题中使用决策树模型有很多的优点，决策树计算复杂度不高、便于使用、而且高效，决策树可处理具有不相关特征的数据、可很容易地构造出易于理解的规则，而规则通常易于解释和理解。</li>
<li>决策树模型也有一些缺点，比如处理缺失数据时的困难、过度拟合以及忽略数据集中属性之间的相关性等。</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[感知器学习算法]]></title>
      <url>http://shuimuth.github.io/2016/05/22/pla/</url>
      <content type="html"><![CDATA[<h1 id="感知器学习算法pla">感知器学习算法（PLA）</h1>
<p>Perceptron（感知器）这个词是机器学习中的一个重要概念，来源于神经科学，是科学家们通过数学的方法来模拟神经元的工作方式而得到的一种简单的机器学习算法。</p>
<h2 id="神经元模型">神经元模型</h2>
<p>对于神经元的简单抽象如图所示,每个神经元是与其他神经元连结在一起的，一个神经元会受到多个其他神经元状态的冲击，并且不同神经元传来的神经冲动对该神经元的影响不同，由一个加权因子 w 来控制，并由此决定自身激发状态。 <a id="more"></a></p>
<div align="center">
<img src="/2016/05/22/pla/neuron_model.jpeg" alt="神经元模型" title="神经元模型">
</div>
<p>在这个模型中神经元的激活值 a 表示为</p>
<p><span class="math display">\[a = \left[\sum_{d=1}^{D}{w_dx_d}\right]+b\]</span></p>
<p>这里的偏移量 b 也是有一定意义的：</p>
<ul>
<li>定义了神经元的激发临界值</li>
<li>在空间上，它对决策边界(decision boundary) 有平移作用，就像常数作用在一次或二次函数上的效果</li>
</ul>
<h2 id="感知器算法">感知器算法</h2>
<p>感知器使用特征向量来表示的前馈式人工神经网络，它是一种二元分类器，把矩阵上的输入 x（实数值向量）映射到输出值 f(x)上（一个二元的值）。</p>
<p><span class="math display">\[
y=
\begin{cases}
1, \quad \text{if} \,\,\, wx + b &gt; 0\\
0, \quad \text{else}
\end{cases}
\]</span></p>
<p>感知器算法是二维线性分类器(Binary Classifier)，但它与我们所知道的决策树算法和 KNN 都不太一样。主要区别在于：</p>
<ul>
<li>感知器算法是一种所谓“错误驱动(error-driven)”的算法。当我们训练这个算法时，只要输出值是正确的，这个算法就不会进行任何数据的调整。反之，当输出值与实际值异号，这个算法就会自动调整参数的比重。</li>
<li>感知器算法是实时(online)的。它逐一处理每一条数据，而不是进行批处理。</li>
</ul>
<h3 id="学习准则">学习准则</h3>
<p>对于感知器学习算法来说，学习准则是比较简单的。只有判别错误的时候才进行更新，更新公式为：</p>
<p><span class="math display">\[ w \leftarrow w_d + yx_d \\
b \leftarrow b + y \]</span></p>
<h2 id="具体实现">具体实现</h2>
<p>下面是感知器学习算法的具体实现。这个算法使用了两个测试数据，一个是老师提供的分类数据，一个是自动生成的多元高斯 分布的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /usr/bin python</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">'wangzx'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">(mean, cov)</span>:</span></span><br><span class="line">    x = []</span><br><span class="line">    y = []</span><br><span class="line">    <span class="keyword">for</span> i, m <span class="keyword">in</span> enumerate(mean):</span><br><span class="line">        x.extend(np.random.multivariate_normal(m, cov[i], (<span class="number">1000</span>,)))</span><br><span class="line">        y.extend([i] * <span class="number">1000</span>)</span><br><span class="line">    g_data = [(f, t) <span class="keyword">for</span> f, t <span class="keyword">in</span> zip(x, y)]</span><br><span class="line">    np.random.shuffle(g_data)</span><br><span class="line">    <span class="keyword">return</span> g_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(g_data, line=None)</span>:</span></span><br><span class="line">    x = [f <span class="keyword">for</span> f, t <span class="keyword">in</span> g_data]</span><br><span class="line">    x = np.asarray(x)</span><br><span class="line">    y = [t <span class="keyword">for</span> f, t <span class="keyword">in</span> g_data]</span><br><span class="line">    y = np.asarray(y)</span><br><span class="line">    plt.scatter(x[:,<span class="number">0</span>], x[:,<span class="number">1</span>], <span class="number">20</span>, y)</span><br><span class="line">    <span class="keyword">if</span> line <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        plt.plot(line[:,<span class="number">0</span>], line[:,<span class="number">1</span>], <span class="string">'g'</span>)</span><br><span class="line">    <span class="comment">#plt.plot([-8,6], [-4, 14], 'g')</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    mean = [[<span class="number">-2</span>,<span class="number">7</span>], [<span class="number">1</span>,<span class="number">1</span>]]</span><br><span class="line">    cov1 = [[<span class="number">1</span>,<span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">3</span>]]</span><br><span class="line">    cov2 = [[<span class="number">1</span>,<span class="number">0</span>], [<span class="number">0</span>,<span class="number">1</span>]]</span><br><span class="line">    g_data = generate_data(mean, [cov1, cov2])</span><br><span class="line">    plot_data(g_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>上面的脚本文件生成了包含两个类标的数据，每一类中的数据都是服从多元高斯分布的，数据 如下图所示。</p>
<div align="center">
<img src="/2016/05/22/pla/cl.png" alt="cl.png" title="">
</div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /usr/bin python</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cPickle <span class="keyword">as</span> pkl</span><br><span class="line"><span class="keyword">from</span> generate_data <span class="keyword">import</span> generate_data, plot_data</span><br><span class="line">__author__ = <span class="string">'wangzx'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" A Perceptron instance can take a function and attempt to</span><br><span class="line">    ``learn`` a bias and set of weights that compute that function,</span><br><span class="line">    using the perceptron learning algorithm."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="string">""" Initialize the perceptron with the bias and all weights</span><br><span class="line">        set to 0.0. ``inputs`` is the input to the</span><br><span class="line">        perceptron."""</span></span><br><span class="line">        num_inputs = inputs.shape[<span class="number">1</span>]</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.bias = <span class="number">0.0</span></span><br><span class="line">        self.weights = np.zeros(num_inputs)</span><br><span class="line">        self.inputs = inputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">output</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">""" Return the output (0 or 1) from the perceptron, with input</span><br><span class="line">        ``x``."""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> np.inner(self.weights, x)+self.bias &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, y, eta=<span class="number">0.1</span>, max_epoch=<span class="number">100</span>)</span>:</span></span><br><span class="line">        <span class="comment"># initialize the bias and weights with random values</span></span><br><span class="line">        self.bias = np.random.normal()</span><br><span class="line">        self.weights = np.random.randn(self.num_inputs)</span><br><span class="line">        number_of_errors = <span class="number">-1</span></span><br><span class="line">        epoch = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> number_of_errors != <span class="number">0</span> <span class="keyword">and</span> epoch &lt; max_epoch:</span><br><span class="line">            number_of_errors = <span class="number">0</span></span><br><span class="line">            epoch += <span class="number">1</span></span><br><span class="line">            <span class="comment">#print("epoch %d" % epoch)</span></span><br><span class="line">            <span class="comment">#print("Beginning iteration")</span></span><br><span class="line">            <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(self.inputs):</span><br><span class="line">                y_pre = self.output(x)</span><br><span class="line">                <span class="keyword">if</span> y[i] != y_pre:</span><br><span class="line">                    number_of_errors += <span class="number">1</span></span><br><span class="line">                    self.bias = self.bias + eta*y[i]</span><br><span class="line">                    self.weights = self.weights + eta*y[i]*x</span><br><span class="line">            <span class="comment">#print("Number of errors:", number_of_errors, "\n")</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        res = [self.output(x) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">        <span class="keyword">return</span> np.asarray(res)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span></span><br><span class="line">    feat = pkl.load(open(<span class="string">"data/train_X.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    y = pkl.load(open(<span class="string">"data/train_Y.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    y[y==<span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line">    test_feat = pkl.load(open(<span class="string">"data/test_X.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    test_y = pkl.load(open(<span class="string">"data/test_Y.pkl"</span>, <span class="string">'rb'</span>))</span><br><span class="line">    test_y[test_y==<span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line">    pla = Perceptron(feat)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Begin fit training data"</span>)</span><br><span class="line">    pla.learn(y, eta=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    y_prd = pla.predict(test_feat)</span><br><span class="line">    score = np.sum(y_prd == test_y) / y_prd.shape[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">"Test accuracy is %f"</span> % score)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_gdata</span><span class="params">()</span>:</span></span><br><span class="line">    mean = [[<span class="number">-2</span>,<span class="number">7</span>], [<span class="number">1</span>,<span class="number">1</span>]]</span><br><span class="line">    cov1 = [[<span class="number">1</span>,<span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">3</span>]]</span><br><span class="line">    cov2 = [[<span class="number">1</span>,<span class="number">0</span>], [<span class="number">0</span>,<span class="number">1</span>]]</span><br><span class="line">    g_data = generate_data(mean, [cov1, cov2])</span><br><span class="line">    X = [f <span class="keyword">for</span> f, t <span class="keyword">in</span> g_data]</span><br><span class="line">    X = np.asarray(X)</span><br><span class="line">    y = [t <span class="keyword">for</span> f, t <span class="keyword">in</span> g_data]</span><br><span class="line">    y = np.asarray(y)</span><br><span class="line">    y[y==<span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    pla = Perceptron(X)</span><br><span class="line">    print(<span class="string">"Begin fit training data"</span>)</span><br><span class="line">    pla.learn(y, eta=<span class="number">1</span>, max_epoch=<span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">    y_prd = pla.predict(X)</span><br><span class="line">    score = np.sum(y_prd == y) / y_prd.shape[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">"Test accuracy is %f"</span> % score)</span><br><span class="line">    border_line(pla, g_data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">border_line</span><span class="params">(pla, g_data)</span>:</span></span><br><span class="line">    b = pla.bias</span><br><span class="line">    w = pla.weights</span><br><span class="line">    x = np.asarray([[<span class="number">-8</span>, <span class="number">6</span>]])</span><br><span class="line">    y = -(w[<span class="number">0</span>]*x + b) / w[<span class="number">1</span>]</span><br><span class="line">    plot_data(g_data, np.concatenate((x, y)).T)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#test()</span></span><br><span class="line">    <span class="comment">#test_gdata()</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>这是在自动生成的数据集中得到的结果，可以看到感知器算法能够比较理想地拟合线性可分的数据（这个数据集并非是线性可分的，有几个数据点交叉了，所以要设定 max_epoch，而不是让它迭代直至收敛）。</p>
<div align="center">
<img src="/2016/05/22/pla/clr.png" alt="clr.png" title="">
</div>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Naive Bayes]]></title>
      <url>http://shuimuth.github.io/2016/05/22/naive-bayes/</url>
      <content type="html"><![CDATA[<h1 id="naive-bayes">Naive Bayes</h1>
<p>这次实验主要使用朴素贝叶斯算法做分类，除了分类外贝叶斯算法还可以用来做回归。使用朴素贝叶斯的一个前提条件是各个事件都是独立的，这一点在实际应用中是必须要记住的</p>
<h2 id="贝叶斯模型">贝叶斯模型</h2>
<p>在贝叶斯模型中我们对信度(beliefs)比较关心，这个从贝叶斯方式来说可以解释为概率。对于某一事件<span class="math inline">\(A\)</span>，我们通常有这个 事件<span class="math inline">\(A\)</span>的先验知识。比如<strong>下雨</strong>这一事件，我们知道在夏天发生的可能性要大于冬天，这就是一种先验知识。 对于事件<span class="math inline">\(A\)</span>和事件<span class="math inline">\(X\)</span>,贝叶斯公式如下： <a id="more"></a> <span class="math display">\[
 P( A | X ) =  \frac{ P(X | A) P(A) } {P(X) }
 \propto P(X | A) P(A)\;\; (\propto \text{is proportional to } )
\]</span></p>
<p>上面公式中的<span class="math inline">\(P(A)\)</span>表示先验概率, <span class="math inline">\(P(A|X)\)</span>表示后验概率。更深入可以参考 Christopher M. Bishop 写的 <strong>Pattern Recognition And Machine Learning</strong></p>
<p>虽然说朴素贝叶斯理论不是太难，介绍到这里也差不多可以了，但是贝叶斯算法在模式识别中应用是非常广泛的。个人觉得有一句话说得非常有道理:</p>
<blockquote>
<p>如果想要学习知识，读；如果想要理解知识，写；如果想要掌握知识，教</p>
</blockquote>
<p>所以下面再多写些例子，这样也可以让自己更好的理解这部分的知识点</p>
<h2 id="抛硬币例子">抛硬币例子</h2>
<p>抛硬币是我们在教科书上接触的最多的一个例子了，只要合适基本都用这个来举例，所以这里也用这个经典的例子。 现在假设我们不知道抛出一枚硬币后不知道出现正面和反面的概率分别是多少，然后看看如何在实验中通过观察到的数据来做出推论。 我们使用<span class="math inline">\(H\)</span>和<span class="math inline">\(T\)</span>来分别表示某次试验中出现正面、反面。现在有个比较有趣的问题是随着试验次数的增多，我们所做出的推论会有哪些变化呢？ 或者更准确一点来说，后验概率是如何随着观测数据的变化而变化的。 下面借助 python 来帮我们完成这一实验</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> IPython.core.pylabtools <span class="keyword">import</span> figsize</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">figsize(<span class="number">11</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> stats</span><br><span class="line"></span><br><span class="line">dist = stats.beta</span><br><span class="line">n_trials = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">15</span>, <span class="number">50</span>, <span class="number">500</span>]</span><br><span class="line">data = stats.bernoulli.rvs(<span class="number">0.5</span>, size=n_trials[<span class="number">-1</span>])</span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># For the already prepared, I'm using Binomial's conj. prior.</span></span><br><span class="line"><span class="keyword">for</span> k, N <span class="keyword">in</span> enumerate(n_trials):</span><br><span class="line">    sx = plt.subplot(len(n_trials) / <span class="number">2</span>, <span class="number">2</span>, k + <span class="number">1</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"$p$, probability of heads"</span>) \</span><br><span class="line">        <span class="keyword">if</span> k <span class="keyword">in</span> [<span class="number">0</span>, len(n_trials) - <span class="number">1</span>] <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line">    plt.setp(sx.get_yticklabels(), visible=<span class="keyword">False</span>)</span><br><span class="line">    heads = data[:N].sum()</span><br><span class="line">    y = dist.pdf(x, <span class="number">1</span> + heads, <span class="number">1</span> + N - heads)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">"observe %d tosses,\n %d heads"</span> % (N, heads))</span><br><span class="line">    plt.fill_between(x, <span class="number">0</span>, y, color=<span class="string">"#348ABD"</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.vlines(<span class="number">0.5</span>, <span class="number">0</span>, <span class="number">4</span>, color=<span class="string">"k"</span>, linestyles=<span class="string">"--"</span>, lw=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    leg = plt.legend()</span><br><span class="line">    leg.get_frame().set_alpha(<span class="number">0.4</span>)</span><br><span class="line">    plt.autoscale(tight=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.suptitle(<span class="string">"Bayesian updating of posterior probabilities"</span>,</span><br><span class="line">             y=<span class="number">1.02</span>,</span><br><span class="line">             fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure>
<div align="center">
<img src="/2016/05/22/naive-bayes/pr.png" alt="pr.png" title="">
</div>
<p>曲线表示的就是后验概率，曲线的宽度和我们推论的不确定性是正比的。从上面的实验结果图可以看出，随着观测数据(试验 次数)的逐渐增多，后验概率也发生了相应的偏移，最终的概率值逐渐逼近<span class="math inline">\(p=0.5\)</span>(使用虚线表示)，并且曲线的宽度逐渐变窄， 也就是不确定性增大。仔细观察曲线发现，曲线的<em>峰值</em>并不总是在<span class="math inline">\(p=0.5\)</span>处出现，其实这也是在情理之中，因为我们假设 了不知道先验概率是多少。实际上如果我们的观测数据比较奇葩，比如 8 次试验，只有 1 次是正面，那我们得出的分布就会出现 很大的偏置(峰值偏离 p<span class="math inline">\(p=0.5\)</span>比较大)。或许我们会有这样的疑问，给出一个试验数据，比如说上面举例的 8 次试验，为什么 估计正面或者反面的概率不是一个具体的数值(eg. <span class="math inline">\(\frac{1}{8}\)</span>)，而是给出一个分布出来呢？其实这也很正常，给出这个 试验，我们又有多大的把握确定其概率就是<span class="math inline">\(\frac{1}{8}\)</span>，而不是其他的呢。所以上面的曲线就是表示有多大的把握确定概 率<span class="math inline">\(p\)</span>就是某一个值的，这也就是贝叶斯的一个比较重要的地方。</p>
<p>说了这么多，看看朴素贝叶斯算法在老师给的数据集上的表现如何吧。下面只给出实现朴素贝叶斯算法的关键代码，具体细节 的地方就省略了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nb</span><span class="params">(train, p_e, test)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    This is an implementation of naive bayes algorithm, which which compute the</span><br><span class="line">    post probability</span><br><span class="line">    :param train: N*D numpy.array. Training set, each row represent an example</span><br><span class="line">    :param p_e: k*D numpy.array, the kth column represent the probability of</span><br><span class="line">    the kth kind of emotion. The dth row represent the probability of the dth</span><br><span class="line">    example.</span><br><span class="line">    :param test: N*D numpy.array. test set, each row represent an example</span><br><span class="line">    :return: 1*N numpy.array. The length of return value is equal to `test.shape[0]`</span><br><span class="line">    """</span></span><br><span class="line">    tmp = train.T.dot(p_e) + <span class="number">1e-3</span></span><br><span class="line">    p_w_given_e = tmp<span class="comment"># / tmp.sum(axis=0)</span></span><br><span class="line">    <span class="comment">#p_w = p_w_given_e.sum(axis=1)</span></span><br><span class="line"></span><br><span class="line">    p_e_pri = p_e.sum(axis=<span class="number">0</span>)<span class="comment"># / p_e.sum()</span></span><br><span class="line">    res = np.zeros(shape=(test.shape[<span class="number">0</span>], p_e.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(p_e.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(test.shape[<span class="number">0</span>]):</span><br><span class="line">            word = test[j]</span><br><span class="line">            w_ind = np.nonzero(word)</span><br><span class="line">            tmp = p_w_given_e[:, i][w_ind].prod() * p_e_pri[i]</span><br><span class="line">            res[j, i] = tmp <span class="comment"># / p_w[i]</span></span><br><span class="line"></span><br><span class="line">    res /= res.sum(axis=<span class="number">1</span>)[:,np.newaxis]</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>以下数据是朴素贝叶斯算法应用在有 1000 多个情感数据集中的表现，其实比 KNN 的要差。对不同情感预测的相关系数如下：</p>
<table>
<thead>
<tr class="header">
<th align="center">anger</th>
<th align="center">disgust</th>
<th align="center">fear</th>
<th align="center">joy</th>
<th align="center">sad</th>
<th align="center">surprised</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.1859</td>
<td align="center">0.1765</td>
<td align="center">0.3034</td>
<td align="center">0.2451</td>
<td align="center">0.2283</td>
<td align="center">0.1490</td>
</tr>
</tbody>
</table>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[30 本名著里浓缩成的 30 句名言]]></title>
      <url>http://shuimuth.github.io/2016/05/06/mingyan/</url>
      <content type="html"><![CDATA[<ol style="list-style-type: decimal">
<li>真正有气质的淑女，从不炫耀她所拥有的一切，她不告诉人她读过什么书，去过什么地方，有多少件衣服， 买过什么珠宝，因为她没有自卑感。 —《圆舞》</li>
<li>最佳的报复不是仇恨，而是打心底发出的冷淡，干嘛花力气去恨一个不相干的人。 —《我的前半生》</li>
<li>无论多豪华的婚礼都不代表幸福婚姻，两个人终生相处和睦与否和筵开几席，多少首饰全无关联。 —《小紫荆》</li>
<li>无论做什么，记得为自己而做，那就毫无怨言。 —《流金岁月》</li>
<li>两个人的适配是一种内心的感觉，而不是一种视觉，千万不要因满足视觉而忽视感觉。 —《花常好月常圆人长久》</li>
<li>我的归宿就是健康与才干，一个人终究可以信赖的，不过是他自己，能够为他扬眉吐气的也是他自己， 我要什么归宿？我已找回我自己，我就是我的归宿。 —《胭脂》</li>
<li>要生活的漂亮，需要付出极大的忍耐，一不抱怨，二不解释，绝对是个人才。 —《变形记》</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[一句话，让你不生气]]></title>
      <url>http://shuimuth.github.io/2016/04/18/%E4%B8%80%E5%8F%A5%E8%AF%9D%EF%BC%8C%E8%AE%A9%E4%BD%A0%E4%B8%8D%E7%94%9F%E6%B0%94/</url>
      <content type="html"><![CDATA[<ul>
<li>有人问余光中：“李敖天天找你茬，你从不回应，这是为什么？” 余沉吟片刻答：“天天骂我，说明他生活不能没有我；而我不搭理，证明我的生活可以没有他。”</li>
<li>有人问毕加索：“你的画怎么看不懂啊？”毕加索说：“听过鸟叫吗？”“听过。”“好听吗？”“好听。”“你听得懂吗？”</li>
</ul>
<p>有人问余光中：“李敖天天找你茬，你从不回应，这是为什么？” 余沉吟片刻答：“天天骂我，说明他生活不能没有我；而我不搭理，证明我的生活可以没有他。”</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Weekly Review Checklist]]></title>
      <url>http://shuimuth.github.io/2015/11/15/weeklyreview/</url>
      <content type="html"><![CDATA[

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> The Weekly Review</h2>
<div class="outline-text-2" id="text-1">
<p>
&quot;If you&apos;re not doing the weekly review, you&apos;re not doing GTD!&quot;
Ricky&apos;s RAM Dump: <a href="http://www.rickyspears.com/blog" target="_blank" rel="external">http://www.rickyspears.com/blog</a>
</p>

<p>
&quot;It&apos;s called a weekly review, not a weakly review!&quot;
</p>

<p>
&quot;Seven days without a review makes one weak!&quot;
</p>

<p>
-&#x2014;
</p>

<p>
Notes from David Allen&apos;s book &quot;Ready for Anything&quot;.
</p>

<p>
The first challenge is to implement these models, and the second is to
keep them active and functional. This guide provides the master key to
achieving a consistently more relaxed and productive style of life and
work. This process, whenever it&apos;s done, facilitates executive
command-center thinking and confidence, and it&apos;s most effective when
it&apos;s practiced every seven days.
</p>

<p>
Charles&apos; action items have been added as checklist items
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Loose Papers <code>[0/5]</code></h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>Gather all scraps of paper, business cards, receipts, and
miscellaneous paper. Put into your in-basket to process.
</li>
<li><code>[&#xA0;]</code> Empty out backpack. Put relevant items into intray.
</li>
<li><code>[&#xA0;]</code> Remove loose stuff from desk
</li>
<li><code>[&#xA0;]</code> Empty wallet of receipts and paper
</li>
<li><code>[&#xA0;]</code> Purge physical intray
</li>
<li><code>[&#xA0;]</code> Empty email inbox creating NAs as required
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Process Your Notes <code>[0/2]</code></h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>Review any journal/notes types of entries, meeting notes, and
miscellaneous notes scribbled on notebook paper. Decide and enter
action items, projects, waiting-fors, etc., as appropriate.
</li>
<li><code>[&#xA0;]</code> Review my Journal file  (C-c j)
</li>
<li><code>[&#xA0;]</code> Review my Office Journal file (linked from Journal File)
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> Empty Your Head <code>[0/1]</code></h2>
<div class="outline-text-2" id="text-4">
<ul class="org-ul">
<li>Put in writing (in appropriate categories) any new projects, action
items, waiting-fors, someday/maybes, etc., not yet captured.
</li>
<li><code>[&#xA0;]</code> Set a timer for 5 minutes and do brainstorming or mind-mapping
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> Review Action Lists <code>[0/1]</code></h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>Mark off completed actions. Review for reminders of further action
steps to record.
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> Review Waiting-For List <code>[0/1]</code></h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>Record appropriate actions for any needed follow-up. Check off
received ones.
</li>
<li><code>[&#xA0;]</code> Review TODO items in Waiting State  C-c a T W &lt;tab&gt;
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> Review Project (and Larger Outcome) Lists <code>[0/2]</code></h2>
<div class="outline-text-2" id="text-7">
<ul class="org-ul">
<li>Evaluate status of projects, goals and outcomes, one by one,
ensuring at least one current action item on each. Browse through
work-in-progress support material to trigger new actions,
completions, waiting-fors, etc.
</li>
<li><code>[&#xA0;]</code> Review Project List  (C-c a P)
</li>
<li><code>[&#xA0;]</code> Review contents of newgtd.org file   (C-c g)
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-8" class="outline-2">
<h2 id="sec-8"><span class="section-number-2">8</span> Review Previous Calendar Data <code>[0/2]</code></h2>
<div class="outline-text-2" id="text-8">
<ul class="org-ul">
<li>Review past calendar in detail for remaining action items, reference
data, etc., and transfer into the active system.
</li>
<li><code>[&#xA0;]</code> Review Logged items for last week (C-c a a l left-arrow)
</li>
<li><code>[&#xA0;]</code> Read newgtd.org<sub>archive</sub> file
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-9" class="outline-2">
<h2 id="sec-9"><span class="section-number-2">9</span> Review Upcoming Calendar <code>[0/2]</code></h2>
<div class="outline-text-2" id="text-9">
<ul class="org-ul">
<li>Review upcoming calendar events-long and short term. Capture actions
triggered.
</li>
<li><code>[&#xA0;]</code> Review upcoming month (C-c a a M)
</li>
<li><code>[&#xA0;]</code> Review family calendar on refrigerator
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-10" class="outline-2">
<h2 id="sec-10"><span class="section-number-2">10</span> Review Any Relevant Checklists <code>[0/3]</code></h2>
<div class="outline-text-2" id="text-10">
<ul class="org-ul">
<li>Use as a trigger for any new actions.
</li>
<li>Review Someday/Maybe List
</li>
<li>Review for any projects that may now have become active, and
transfer to projects list. Delete items no longer of interest.
</li>
<li><code>[&#xA0;]</code> Review my <a href="someday.html">Someday/Maybe</a> list
</li>
<li><code>[&#xA0;]</code> Write a short summary of the week (C-c r a)
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-11" class="outline-2">
<h2 id="sec-11"><span class="section-number-2">11</span> Be Creative and Courageous <code>[0/1]</code></h2>
<div class="outline-text-2" id="text-11">
<ul class="org-ul">
<li>Any new, wonderful, harebrained, creative, thought-provoking,
risk-taking ideas to add into your system???
</li>
<li><code>[&#xA0;]</code> Capture Todo items (C-c r t) or Someday (C-c r s)
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-12" class="outline-2">
<h2 id="sec-12"><span class="section-number-2">12</span> Final Administration <code>[0/2]</code></h2>
<div class="outline-text-2" id="text-12">
<ul class="org-ul">
<li><code>[&#xA0;]</code> Start Quicken and get my finances up to date
</li>
<li><code>[&#xA0;]</code> Backup GTD File
</li>
<li>england
</li>
<li>finances
</li>
<li><a href="http://www.emacswiki.org/cgi-bin/wiki.pl" target="_blank" rel="external">http://www.emacswiki.org/cgi-bin/wiki.pl</a>
</li>
<li>as far as I can tell
</li>
</ul>
</div>
</div>

Last Updated 2016-05-19 &#x56DB; 13:02.<br>Render by <a href="https://github.com/CodeFalling/hexo-renderer-org" target="_blank" rel="external">hexo-renderer-org</a> with <a href="http://www.gnu.org/software/emacs/" target="_blank" rel="external">Emacs</a> 24.5.2 (<a href="http://orgmode.org" target="_blank" rel="external">Org</a> mode 8.2.10)
]]></content>
    </entry>
    
  
  
    
    <entry>
      <title><![CDATA[关于]]></title>
      <url>http://shuimuth.github.io/about/index.html</url>
      <content type="html"><![CDATA[<p>大家好，我是XXX。欢迎来到我的个人技术博客。</p>
<p>这里用markdown写下你的简介，就跟平时写md一样就可以了。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[读书]]></title>
      <url>http://shuimuth.github.io/reading/index.html</url>
      <content type="html"><![CDATA[
]]></content>
    </entry>
    
  
</search>
